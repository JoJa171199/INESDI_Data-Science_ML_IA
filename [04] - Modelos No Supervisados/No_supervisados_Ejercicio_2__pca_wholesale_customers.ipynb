{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/INESDI_Data-Science_ML_IA/blob/main/%5B04%5D%20-%20Modelos%20No%20Supervisados/No_supervisados_Ejercicio_2__pca_wholesale_customers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a789fd1",
      "metadata": {
        "id": "4a789fd1"
      },
      "source": [
        "# No supervisados - Ejercicio 2: pca_wholesale_customers.ipynb\n",
        "\n",
        "Este notebook es **We do**. Lo trabajamos juntos en clase: verás celdas resueltas que explican los pasos más técnicos y celdas con comentarios `# TODO` que debes completar. Ejecuta las celdas en orden y usa las guías dentro del código para terminar las partes interactivas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5cafa6",
      "metadata": {
        "id": "4a5cafa6"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "- Cargar y explorar el dataset *Wholesale Customers*.\n",
        "- Preparar las features de gasto y estandarizarlas.\n",
        "- Calcular PCA, analizar la varianza explicada y elegir número de componentes para retener 90% de varianza.\n",
        "- Reconstruir datos desde componentes y medir el error.\n",
        "- Comparar el efecto de PCA sobre KMeans (antes/después) y discutir resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ba5e9f7",
      "metadata": {
        "id": "0ba5e9f7"
      },
      "outputs": [],
      "source": [
        "# Librerías y configuración (resuelto)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import mean_squared_error, silhouette_score\n",
        "\n",
        "np.random.seed(42)\n",
        "print('Librerías importadas')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f28a6d9a",
      "metadata": {
        "id": "f28a6d9a"
      },
      "source": [
        "### Carga de datos\n",
        "\n",
        "Carga el CSV desde la URL si puedes, o sube el archivo al entorno. A continuación tienes un ejemplo comentado y una celda `# TODO` para que hagas la carga."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a36c1b9",
      "metadata": {
        "id": "0a36c1b9"
      },
      "outputs": [],
      "source": [
        "# TODO: Carga el dataset Wholesale Customers en un DataFrame `df`.\n",
        "# Opciones:\n",
        "# url = 'https://raw.githubusercontent.com/dtoralg/INESDI_Data-Science_ML_IA/refs/heads/main/%5B04%5D%20-%20Modelos%20No%20Supervisados/Wholesale%20customers%20data.csv'\n",
        "# df = pd.read_csv(url)\n",
        "# display(df.head())\n",
        "# print(df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7037f2a6",
      "metadata": {
        "id": "7037f2a6"
      },
      "source": [
        "### Exploración rápida\n",
        "\n",
        "Revisa tipos, nulos y estadísticas. Observa la estructura y confirma las columnas de gasto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f608ed",
      "metadata": {
        "id": "12f608ed"
      },
      "outputs": [],
      "source": [
        "# Ejecuta esto si ya has cargado `df` (resuelto)\n",
        "try:\n",
        "    display(df.head())\n",
        "    print('\\nInfo:')\n",
        "    print(df.info())\n",
        "    print('\\n% de nulos por columna:')\n",
        "    display((df.isnull().mean()*100).round(2))\n",
        "    print('\\nDescripción numérica:')\n",
        "    display(df.select_dtypes(include=[np.number]).describe().T)\n",
        "except NameError:\n",
        "    print('Carga el dataset en la celda de carga antes de ejecutar esta celda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50454982",
      "metadata": {
        "id": "50454982"
      },
      "source": [
        "### Selección de features\n",
        "\n",
        "Usaremos las columnas de gasto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d38debb",
      "metadata": {
        "id": "0d38debb"
      },
      "outputs": [],
      "source": [
        "# Resuelto: identifico columnas numéricas y propongo features\n",
        "# Ajusta la lista si tu CSV tiene nombres distintos\n",
        "try:\n",
        "    vars_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    print('Columnas numéricas detectadas:', vars_numericas)\n",
        "except NameError:\n",
        "    print('df no definido. Carga el dataset primero.')\n",
        "\n",
        "# TODO: Define `features` con las columnas de gasto (p. ej. ['Fresh','Milk','Grocery','Frozen','Detergents_Paper','Delicassen'])\n",
        "# Crea X = df[features].copy() y muestra X.describe().T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc10370f",
      "metadata": {
        "id": "bc10370f"
      },
      "source": [
        "### Escalado\n",
        "\n",
        "Estandariza las features antes de PCA. El scaler está creado; completa la aplicación sobre `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77935fcd",
      "metadata": {
        "id": "77935fcd"
      },
      "outputs": [],
      "source": [
        "# Resuelto: creamos el scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# TODO: aplica scaler sobre X y guarda X_scaled como DataFrame con las mismas columnas\n",
        "# Ejemplo guía (hazlo en clase):\n",
        "# X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=features)\n",
        "# display(X_scaled.describe().T)\n",
        "\n",
        "try:\n",
        "    X  # comprobación\n",
        "except NameError:\n",
        "    print('Define X en la celda de selección de features antes de escalar')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2747923c",
      "metadata": {
        "id": "2747923c"
      },
      "source": [
        "### PCA: cálculo y varianza explicada\n",
        "\n",
        "Ajusta PCA y muestra la varianza explicada. Elige el `target_var` (ej. 0.90) en la celda siguiente y calcula cuántas componentes son necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8264c36",
      "metadata": {
        "id": "e8264c36"
      },
      "outputs": [],
      "source": [
        "# Resuelto: cálculo inicial de PCA (requiere X_scaled)\n",
        "try:\n",
        "    pca_full = PCA()\n",
        "    X_pca_full = pca_full.fit_transform(X_scaled)\n",
        "    explained = pca_full.explained_variance_ratio_\n",
        "    cumulative = np.cumsum(explained)\n",
        "    explained_df = pd.DataFrame({'pc': np.arange(1, len(explained)+1), 'explained_variance': explained, 'cumulative': cumulative})\n",
        "    explained_df.index = explained_df['pc']\n",
        "    display(explained_df[['explained_variance','cumulative']])\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(explained_df['pc'], explained_df['explained_variance'], marker='o', label='Individual')\n",
        "    plt.plot(explained_df['pc'], explained_df['cumulative'], marker='o', label='Cumulative')\n",
        "    plt.xlabel('Componente principal')\n",
        "    plt.ylabel('Varianza explicada')\n",
        "    plt.xticks(explained_df['pc'])\n",
        "    plt.legend()\n",
        "    plt.title('Scree plot y varianza acumulada')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # TODO: en clase elige target_var (ej. 0.90) y calcula n_components_90 = np.argmax(cumulative >= target_var) + 1\n",
        "    # Luego crea pca90 = PCA(n_components=n_components_90) y X_pca_90 = pca90.fit_transform(X_scaled)\n",
        "\n",
        "except Exception as e:\n",
        "    print('Asegúrate de tener X_scaled definido. Detalle:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "276e238c",
      "metadata": {
        "id": "276e238c"
      },
      "source": [
        "### Reconstrucción y error (guía)\n",
        "\n",
        "Vamos a medir la pérdida de información al reconstruir los datos desde un número reducido de componentes. Completa el bucle para k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91e4650",
      "metadata": {
        "id": "e91e4650"
      },
      "outputs": [],
      "source": [
        "# Ejemplo resuelto parcial y guía para completar\n",
        "try:\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    # Ejemplo: reconstrucción y MSE con k=2 (modifica en clase)\n",
        "    k_example = min(2, X_scaled.shape[1])\n",
        "    pca_k = PCA(n_components=k_example)\n",
        "    Xk = pca_k.fit_transform(X_scaled)\n",
        "    Xk_rec = pca_k.inverse_transform(Xk)\n",
        "    mse_example = mean_squared_error(X_scaled, Xk_rec)\n",
        "    print(f'MSE reconstrucción con k={k_example}:', mse_example)\n",
        "\n",
        "    # TODO: recorre k desde 1 hasta X_scaled.shape[1], calcula MSE para cada k y guarda resultados en mse_df. Dibuja MSE vs k.\n",
        "\n",
        "except NameError:\n",
        "    print('Asegúrate de definir X_scaled antes de ejecutar esta celda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2208bc67",
      "metadata": {
        "id": "2208bc67"
      },
      "source": [
        "### PCA antes/después de clustering\n",
        "\n",
        "Queremos comparar cómo cambia la inercia de KMeans al aplicar PCA antes del clustering. Completa la comparación y revisa cuándo tiene sentido usar PCA como preprocesado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be296ad",
      "metadata": {
        "id": "1be296ad"
      },
      "outputs": [],
      "source": [
        "# Guía para la comparativa (resuelto parcialmente)\n",
        "# TODO: completa estos pasos en clase:\n",
        "# - define k_chosen (por ejemplo 4 o el valor que acordemos)\n",
        "# - km_orig = KMeans(n_clusters=k_chosen, random_state=42, n_init=10).fit(X_scaled); inertia_orig = km_orig.inertia_\n",
        "# - km_pca = KMeans(n_clusters=k_chosen, random_state=42, n_init=10).fit(X_pca_90) (si has calculado X_pca_90)\n",
        "# - imprime inertia_orig e inertia_pca y comenta la diferencia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d476e52",
      "metadata": {
        "id": "3d476e52"
      },
      "source": [
        "### Interpretación y entrega\n",
        "\n",
        "Redacta en la última celda un resumen (3–5 líneas) con el número de componentes que decidiste retener, el error de reconstrucción observado y si recomendarías usar PCA antes de clustering en este caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8757247",
      "metadata": {
        "id": "c8757247"
      },
      "outputs": [],
      "source": [
        "# Escribe tu resumen final aquí (sustituye el texto entre comillas):\n",
        "resumen = '''\n",
        "\n",
        "'''\n",
        "print(resumen)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}