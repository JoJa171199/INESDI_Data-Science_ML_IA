{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/INESDI_Data-Science_ML_IA/blob/main/%5B02%5D%20-%20Modelos%20Supervisados%20Lineales/Supervisados_Lineales_Ejercicio_7_naive_bayes_diabetes_EXTENDIDO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7381fe92",
      "metadata": {
        "id": "7381fe92"
      },
      "source": [
        "## Supervisados Lineales - Ejercicio 7 (extendido): naive_bayes_diabetes.ipynb\n",
        "\n",
        "En este ejercicio abordaremos **tres variantes de Naive Bayes** aplicadas al dataset **Diabetes** de `scikit-learn`:\n",
        "- **GaussianNB**: asume rasgos **continuos** con distribución normal condicionada a la clase.\n",
        "- **CategoricalNB**: requiere rasgos **discretos/categóricos**; discretizaremos (binning) las variables continuas.\n",
        "- **MultinomialNB**: diseñado para **conteos** (no negativos); reescalaremos a [0, 1] y discutiremos limitaciones.\n",
        "\n",
        "Formularemos un problema de **clasificación binaria** a partir del objetivo continuo (progresión de la enfermedad), evaluaremos con **accuracy, ROC-AUC, matriz de confusión** y cerraremos con una **comparativa** clara de resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05b6484",
      "metadata": {
        "id": "f05b6484"
      },
      "source": [
        "### Objetivos\n",
        "- **O1.** Entrenar y evaluar **GaussianNB**, **CategoricalNB** y **MultinomialNB**.\n",
        "- **O2.** Comprender cómo las **suposiciones** de cada variante afectan el rendimiento.\n",
        "- **O3.** Comparar resultados con métricas y **matrices de confusión**; razonar cuándo usar cada enfoque."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "779dccfa",
      "metadata": {
        "id": "779dccfa"
      },
      "source": [
        "### Dataset\n",
        "El dataset **Diabetes** contiene 442 pacientes y 10 predictores normalizados (edad, sexo, IMC, presión sanguínea y 6 medidas bioquímicas). El objetivo original es **continuo** (progresión al año). Para clasificación, lo **binarizaremos por la mediana**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12df5d7a",
      "metadata": {
        "id": "12df5d7a"
      },
      "source": [
        "### 1. Importación de librerías y utilidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f7ecf8",
      "metadata": {
        "id": "63f7ecf8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, CategoricalNB, MultinomialNB\n",
        "from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "def plot_confusion(cm, title, ax=None):\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(4,3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Pred 0','Pred 1'], yticklabels=['Real 0','Real 1'], ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Predicción')\n",
        "    ax.set_ylabel('Real')\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a037d5",
      "metadata": {
        "id": "e8a037d5"
      },
      "source": [
        "### 2. Carga de datos y binarización del objetivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d2e99c",
      "metadata": {
        "id": "14d2e99c"
      },
      "outputs": [],
      "source": [
        "# Cargar dataset de Diabetes\n",
        "data = load_diabetes()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y_cont = pd.Series(data.target, name='progression')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarizamos el objetivo: 1 si progresión > mediana, 0 en caso contrario\n",
        "thr = y_cont.median()\n",
        "y = (y_cont > thr).astype(int)\n",
        "print('Umbral de binarización (mediana):', thr)\n",
        "display(X.head())\n",
        "X.shape, y.value_counts(normalize=True).round(3)"
      ],
      "metadata": {
        "id": "40F0o7Ewm8uM"
      },
      "id": "40F0o7Ewm8uM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "68a621c5",
      "metadata": {
        "id": "68a621c5"
      },
      "source": [
        "### 3. División en train/test (estratificada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42a536b",
      "metadata": {
        "id": "e42a536b"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "X_train.shape, X_test.shape, y_train.mean().round(3), y_test.mean().round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b81ef39",
      "metadata": {
        "id": "9b81ef39"
      },
      "source": [
        "### 4. Gaussian Naive Bayes (rasgos continuos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ef4229",
      "metadata": {
        "id": "98ef4229"
      },
      "source": [
        "**Idea:** Modela cada rasgo continuo condicionalmente como una Normal $\\mathcal{N}(\\mu, \\sigma^2)$ por clase. Conviene cuando los rasgos son aproximadamente gaussianos o han sido estandarizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec7f774",
      "metadata": {
        "id": "4ec7f774"
      },
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_gnb = gnb.predict(X_test)\n",
        "y_proba_gnb = gnb.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "4lew_XblnIEI"
      },
      "id": "4lew_XblnIEI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_gnb = accuracy_score(y_test, y_pred_gnb)\n",
        "auc_gnb = roc_auc_score(y_test, y_proba_gnb)\n",
        "cm_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
        "print(f'GaussianNB -> Accuracy: {acc_gnb:.3f} | ROC-AUC: {auc_gnb:.3f}')\n",
        "print(classification_report(y_test, y_pred_gnb, digits=3))\n",
        "plot_confusion(cm_gnb, 'GaussianNB'); plt.show()"
      ],
      "metadata": {
        "id": "KtSJSsAVnJ69"
      },
      "id": "KtSJSsAVnJ69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fe683f56",
      "metadata": {
        "id": "fe683f56"
      },
      "source": [
        "### 5. Categorical Naive Bayes (discretización de rasgos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a5ad7b",
      "metadata": {
        "id": "30a5ad7b"
      },
      "source": [
        "**Idea:** Convertimos rasgos continuos en **categorías** (bins). El modelo aprende $P(x_j = \\text{categoría} \\mid y)$ por cada rasgo. Se pierde granularidad, pero el ajuste a la suposición categórica puede mejorar según el caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398557eb",
      "metadata": {
        "id": "398557eb"
      },
      "outputs": [],
      "source": [
        "# Discretizamos en 5 bins equipoblados (quantile). encode='ordinal' produce categorías [0..n_bins-1]\n",
        "kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
        "X_train_disc = kbd.fit_transform(X_train)\n",
        "X_test_disc  = kbd.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnb = CategoricalNB()\n",
        "cnb.fit(X_train_disc, y_train)"
      ],
      "metadata": {
        "id": "GwEELqLanOVc"
      },
      "id": "GwEELqLanOVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_cnb = cnb.predict(X_test_disc)\n",
        "y_proba_cnb = cnb.predict_proba(X_test_disc)[:, 1]"
      ],
      "metadata": {
        "id": "Z7h2eok3nPoY"
      },
      "id": "Z7h2eok3nPoY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_cnb = accuracy_score(y_test, y_pred_cnb)\n",
        "auc_cnb = roc_auc_score(y_test, y_proba_cnb)\n",
        "cm_cnb = confusion_matrix(y_test, y_pred_cnb)\n",
        "print(f'CategoricalNB -> Accuracy: {acc_cnb:.3f} | ROC-AUC: {auc_cnb:.3f}')\n",
        "print(classification_report(y_test, y_pred_cnb, digits=3))\n",
        "plot_confusion(cm_cnb, 'CategoricalNB'); plt.show()"
      ],
      "metadata": {
        "id": "J0qj_sQ-nQpp"
      },
      "id": "J0qj_sQ-nQpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dea04b4e",
      "metadata": {
        "id": "dea04b4e"
      },
      "source": [
        "### 6. Multinomial Naive Bayes (rasgos no negativos / tipo conteo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e0fafb",
      "metadata": {
        "id": "c9e0fafb"
      },
      "source": [
        "**Idea:** Pensado para **conteos** (p. ej., frecuencias de palabras). Requiere rasgos **no negativos**. Aquí haremos un **reescaleo Min–Max a [0, 1]**.\n",
        "\n",
        "**Nota:** Los rasgos originales no son conteos; MultinomialNB puede rendir peor si la inductiva no encaja, pero sirve para mostrar el contraste entre variantes de NB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c26b90c",
      "metadata": {
        "id": "9c26b90c"
      },
      "outputs": [],
      "source": [
        "mms = MinMaxScaler()\n",
        "X_train_mnb = mms.fit_transform(X_train)\n",
        "X_test_mnb  = mms.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_mnb, y_train)"
      ],
      "metadata": {
        "id": "0RcrRUvmnh9Q"
      },
      "id": "0RcrRUvmnh9Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_mnb = mnb.predict(X_test_mnb)\n",
        "y_proba_mnb = mnb.predict_proba(X_test_mnb)[:, 1]"
      ],
      "metadata": {
        "id": "phV1iYYGnjf_"
      },
      "id": "phV1iYYGnjf_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_mnb = accuracy_score(y_test, y_pred_mnb)\n",
        "auc_mnb = roc_auc_score(y_test, y_proba_mnb)\n",
        "cm_mnb = confusion_matrix(y_test, y_pred_mnb)\n",
        "print(f'MultinomialNB -> Accuracy: {acc_mnb:.3f} | ROC-AUC: {auc_mnb:.3f}')\n",
        "print(classification_report(y_test, y_pred_mnb, digits=3))\n",
        "plot_confusion(cm_mnb, 'MultinomialNB'); plt.show()"
      ],
      "metadata": {
        "id": "ijIcol_Znl_a"
      },
      "id": "ijIcol_Znl_a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "38269518",
      "metadata": {
        "id": "38269518"
      },
      "source": [
        "### 7. Comparativa: matrices de confusión y métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75fe7678",
      "metadata": {
        "id": "75fe7678"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(14,4))\n",
        "plot_confusion(cm_gnb, f'GaussianNB\\nAcc={acc_gnb:.2f}', ax=axes[0])\n",
        "plot_confusion(cm_cnb, f'CategoricalNB\\nAcc={acc_cnb:.2f}', ax=axes[1])\n",
        "plot_confusion(cm_mnb, f'MultinomialNB\\nAcc={acc_mnb:.2f}', ax=axes[2])\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "metrics = pd.DataFrame({\n",
        "    'modelo': ['GaussianNB','CategoricalNB','MultinomialNB'],\n",
        "    'accuracy': [acc_gnb, acc_cnb, acc_mnb],\n",
        "    'roc_auc': [auc_gnb, auc_cnb, auc_mnb]\n",
        "}).sort_values('accuracy', ascending=False)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40b7863",
      "metadata": {
        "id": "d40b7863"
      },
      "source": [
        "### 8. Curvas ROC (opcional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43945a49",
      "metadata": {
        "id": "43945a49"
      },
      "outputs": [],
      "source": [
        "fpr_g, tpr_g, _ = roc_curve(y_test, y_proba_gnb)\n",
        "fpr_c, tpr_c, _ = roc_curve(y_test, y_proba_cnb)\n",
        "fpr_m, tpr_m, _ = roc_curve(y_test, y_proba_mnb)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr_g, tpr_g, label=f'GaussianNB AUC={auc_gnb:.3f}')\n",
        "plt.plot(fpr_c, tpr_c, label=f'CategoricalNB AUC={auc_cnb:.3f}')\n",
        "plt.plot(fpr_m, tpr_m, label=f'MultinomialNB AUC={auc_mnb:.3f}')\n",
        "plt.plot([0,1],[0,1],'k--', linewidth=1)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curvas ROC - Comparativa')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4fd5c42",
      "metadata": {
        "id": "b4fd5c42"
      },
      "source": [
        "### Conclusiones\n",
        "- **GaussianNB** suele ser una buena línea base cuando los rasgos son continuos y no presentan distribuciones muy patológicas.\n",
        "- **CategoricalNB** puede mejorar si la discretización captura bien patrones por rango y reduce ruido/heteroscedasticidad.\n",
        "- **MultinomialNB** es apropiado para **conteos**; en datos continuos reescalados, su inductiva no siempre encaja y puede rendir peor.\n",
        "\n",
        "Al comparar **matrices de confusión**, presta atención a:\n",
        "- **Falsos positivos** vs **falsos negativos** según lo que te preocupe minimizar.\n",
        "- Cambios en **recall** y **precision** de la clase positiva al pasar de continuo→discreto→multinomial.\n",
        "\n",
        "Elige el modelo no solo por **accuracy/AUC**, sino por la **distribución de errores** y las **suposiciones** que mejor se ajusten a tu caso."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}