{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnc3UOfdQY6GvENlbX4RJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/INESDI_Data-Science_ML_IA/blob/main/%5B07%5D%20-%20ML%20en%20la%20Empresa/ML_en_la_empresa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎯 Caso Práctico: Industrializar un modelo\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 📋 Información del Módulo\n",
        "\n",
        "**Asignatura:** Data Analytics: Data Science, Machine Learning e Inteligencia Artificial  \n",
        "**Máster:** FP en Business Analytics e Inteligencia Artificial  \n",
        "**Profesores:** Álvaro López Barberá\n",
        "**Ejemplo Práctico:**  ML en la empresa\n",
        "\n",
        "---\n",
        "\n",
        "## 🎓 Objetivo\n",
        "\n",
        "Desarrollar un modelo de lenguaje natural sobre Machine Learning, que nos conteste a nuestras preguntas."
      ],
      "metadata": {
        "id": "6TW047jpphEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 📦 PASO 1: Instalar dependencias y Configuración Inicial"
      ],
      "metadata": {
        "id": "-YLy5GZmp47H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn fastapi uvicorn pydantic joblib requests numpy nest-asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LgZoJy3qlxR",
        "outputId": "e50e2596-3469-4ba7-f7ca-e475d8308cfd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.118.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.37.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')  # Descarga todo (tarda ~5 min)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdNzkCoSgawz",
        "outputId": "3d6e8390-ba98-4a81-9906-0931e0c20571"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3EyuHuSHgnS8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar recursos de NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(\"✅ Recursos descargados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2bxHxeFrhHC",
        "outputId": "d0d6bd95-9b5f-4a15-da1c-060a8f5c1c5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Recursos descargados\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 📊 PASO 2: Crear Base de Conocimiento\n",
        "\n",
        "**Salida esperada:**\n",
        "```\n",
        "CONSTRUYENDO BASE DE CONOCIMIENTO\n",
        "✓ Total de preguntas en la base: 45\n",
        "✓ Vectorizando preguntas con TF-IDF...\n",
        "✓ Vectorización completada\n",
        "💾 Guardando chatbot en chatbot_data.pkl...\n",
        "✅ Chatbot guardado exitosamente"
      ],
      "metadata": {
        "id": "NOmv4rAaqBf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 📊 PASO 2: Crear Base de Conocimiento\n",
        "\n",
        "### Archivo: `01_crear_base_conocimiento.py`\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "PASO 1: CREAR BASE DE CONOCIMIENTO DEL CHATBOT\n",
        "Creamos una base de preguntas-respuestas sobre Machine Learning\n",
        "y la preparamos para calcular similitudes\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURACIÓN INICIAL: Descargar todos los recursos de NLTK\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONFIGURACIÓN INICIAL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nDescargando recursos de NLTK (puede tardar unos segundos)...\")\n",
        "recursos_nltk = ['punkt', 'punkt_tab', 'stopwords', 'wordnet', 'omw-1.4']\n",
        "\n",
        "for recurso in recursos_nltk:\n",
        "    try:\n",
        "        nltk.download(recurso, quiet=True)\n",
        "        print(f\"  ✓ {recurso} descargado\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠ Error descargando {recurso}: {e}\")\n",
        "\n",
        "print(\"\\n✅ Recursos de NLTK listos\\n\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class ChatbotMLFAQ:\n",
        "    \"\"\"\n",
        "    Chatbot simple basado en similitud de texto para FAQs de Machine Learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Base de conocimiento: preguntas y respuestas sobre ML\n",
        "        self.qa_pairs = [\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es machine learning?\",\n",
        "                    \"¿Qué es el aprendizaje automático?\",\n",
        "                    \"Define machine learning\",\n",
        "                    \"Explica qué es ML\"\n",
        "                ],\n",
        "                \"respuesta\": \"Machine Learning es una rama de la Inteligencia Artificial que permite a las máquinas aprender de los datos sin ser programadas explícitamente. Los algoritmos identifican patrones y toman decisiones basándose en ejemplos anteriores.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es overfitting?\",\n",
        "                    \"¿Qué es el sobreajuste?\",\n",
        "                    \"Explica overfitting\",\n",
        "                    \"Qué significa sobreajustar\"\n",
        "                ],\n",
        "                \"respuesta\": \"Overfitting ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido. Esto hace que funcione muy bien con los datos de entrenamiento pero mal con datos nuevos. Se soluciona con validación cruzada, regularización o más datos.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es una red neuronal?\",\n",
        "                    \"Define red neuronal\",\n",
        "                    \"Explica las redes neuronales\",\n",
        "                    \"Qué son las neural networks\"\n",
        "                ],\n",
        "                \"respuesta\": \"Una red neuronal es un modelo de aprendizaje profundo inspirado en el cerebro humano. Está compuesta por capas de neuronas artificiales conectadas que procesan información. Se usa para tareas complejas como reconocimiento de imágenes o procesamiento de lenguaje natural.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué diferencia hay entre clasificación y regresión?\",\n",
        "                    \"Clasificación vs regresión\",\n",
        "                    \"Diferencia entre clasificación y regresión\",\n",
        "                    \"Cuándo usar clasificación o regresión\"\n",
        "                ],\n",
        "                \"respuesta\": \"La clasificación predice categorías o clases (ej: spam/no spam, gato/perro), mientras que la regresión predice valores numéricos continuos (ej: precio de una casa, temperatura). Clasificación da etiquetas, regresión da números.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es el accuracy?\",\n",
        "                    \"Define accuracy\",\n",
        "                    \"Qué mide el accuracy\",\n",
        "                    \"Explica la métrica accuracy\"\n",
        "                ],\n",
        "                \"respuesta\": \"Accuracy (precisión) es el porcentaje de predicciones correctas sobre el total de predicciones. Se calcula como: (predicciones correctas / total de predicciones) × 100. Es útil cuando las clases están balanceadas, pero puede engañar con datos desbalanceados.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es gradient descent?\",\n",
        "                    \"Explica gradient descent\",\n",
        "                    \"Qué es el descenso de gradiente\",\n",
        "                    \"Cómo funciona gradient descent\"\n",
        "                ],\n",
        "                \"respuesta\": \"Gradient Descent es un algoritmo de optimización que ajusta los parámetros de un modelo para minimizar el error. Funciona calculando el gradiente (derivada) de la función de pérdida y moviéndose en la dirección opuesta para encontrar el mínimo.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es cross-validation?\",\n",
        "                    \"Explica la validación cruzada\",\n",
        "                    \"Para qué sirve cross-validation\",\n",
        "                    \"Qué es k-fold\"\n",
        "                ],\n",
        "                \"respuesta\": \"Cross-validation es una técnica para evaluar modelos dividing los datos en k partes (folds). Se entrena k veces usando k-1 partes para entrenar y 1 para validar, rotando. Esto da una estimación más robusta del rendimiento real del modelo.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es un Random Forest?\",\n",
        "                    \"Explica Random Forest\",\n",
        "                    \"Cómo funciona Random Forest\",\n",
        "                    \"Qué es bosque aleatorio\"\n",
        "                ],\n",
        "                \"respuesta\": \"Random Forest es un modelo ensemble que combina múltiples árboles de decisión. Cada árbol se entrena con una muestra aleatoria de datos y features. La predicción final es el promedio (regresión) o voto mayoritario (clasificación) de todos los árboles.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es feature engineering?\",\n",
        "                    \"Explica feature engineering\",\n",
        "                    \"Para qué sirve feature engineering\",\n",
        "                    \"Qué es ingeniería de características\"\n",
        "                ],\n",
        "                \"respuesta\": \"Feature Engineering es el proceso de crear nuevas características (features) a partir de los datos existentes para mejorar el rendimiento del modelo. Incluye transformaciones, combinaciones, extracciones y selección de las variables más relevantes.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es supervised learning?\",\n",
        "                    \"Explica aprendizaje supervisado\",\n",
        "                    \"Qué es supervised learning\",\n",
        "                    \"Diferencia entre supervisado y no supervisado\"\n",
        "                ],\n",
        "                \"respuesta\": \"Supervised Learning es cuando entrenamos un modelo con datos etiquetados (sabemos la respuesta correcta). El modelo aprende la relación entre inputs y outputs. Ejemplos: clasificación de emails, predicción de precios. Se diferencia del no supervisado que trabaja sin etiquetas.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¿Qué es un hiperparámetro?\",\n",
        "                    \"Diferencia entre parámetro e hiperparámetro\",\n",
        "                    \"Explica hiperparámetros\",\n",
        "                    \"Qué son los hyperparameters\"\n",
        "                ],\n",
        "                \"respuesta\": \"Los hiperparámetros son configuraciones del modelo que definimos ANTES del entrenamiento (ej: learning rate, número de capas). Los parámetros son los valores que el modelo APRENDE durante el entrenamiento (ej: pesos de una red neuronal).\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"Hola\",\n",
        "                    \"Buenos días\",\n",
        "                    \"Buenas tardes\",\n",
        "                    \"Hey\"\n",
        "                ],\n",
        "                \"respuesta\": \"¡Hola! Soy un chatbot especializado en Machine Learning. Puedo responder preguntas sobre conceptos de ML, algoritmos, métricas y más. ¿En qué puedo ayudarte?\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"Gracias\",\n",
        "                    \"Muchas gracias\",\n",
        "                    \"Perfecto gracias\",\n",
        "                    \"Ok gracias\"\n",
        "                ],\n",
        "                \"respuesta\": \"¡De nada! Si tienes más preguntas sobre Machine Learning, estaré encantado de ayudarte. 😊\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"Adiós\",\n",
        "                    \"Hasta luego\",\n",
        "                    \"Chao\",\n",
        "                    \"Nos vemos\"\n",
        "                ],\n",
        "                \"respuesta\": \"¡Hasta pronto! Que tengas un buen día aprendiendo Machine Learning. 🚀\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Preparar lematizador y stopwords\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "        # Vectorizador TF-IDF\n",
        "        self.vectorizer = None\n",
        "        self.question_vectors = None\n",
        "        self.all_questions = []\n",
        "        self.question_to_answer = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesa el texto: tokeniza, elimina stopwords y lematiza\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convertir a minúsculas\n",
        "            text = text.lower()\n",
        "\n",
        "            # Tokenizar\n",
        "            tokens = word_tokenize(text, language='spanish')\n",
        "\n",
        "            # Eliminar stopwords y lematizar\n",
        "            processed_tokens = [\n",
        "                self.lemmatizer.lemmatize(token)\n",
        "                for token in tokens\n",
        "                if token.isalnum() and token not in self.stop_words\n",
        "            ]\n",
        "\n",
        "            return ' '.join(processed_tokens)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Si falla la tokenización en español, intentar con inglés\n",
        "            print(f\"⚠ Advertencia en preprocesamiento: {e}\")\n",
        "            try:\n",
        "                tokens = word_tokenize(text)\n",
        "                processed_tokens = [\n",
        "                    self.lemmatizer.lemmatize(token)\n",
        "                    for token in tokens\n",
        "                    if token.isalnum()\n",
        "                ]\n",
        "                return ' '.join(processed_tokens)\n",
        "            except:\n",
        "                # Último recurso: dividir por espacios\n",
        "                return ' '.join(text.lower().split())\n",
        "\n",
        "    def build_knowledge_base(self):\n",
        "        \"\"\"\n",
        "        Construye la base de conocimiento y vectoriza las preguntas\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"CONSTRUYENDO BASE DE CONOCIMIENTO\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Crear lista de todas las preguntas y mapeo a respuestas\n",
        "        for qa_pair in self.qa_pairs:\n",
        "            respuesta = qa_pair[\"respuesta\"]\n",
        "            for pregunta in qa_pair[\"preguntas\"]:\n",
        "                pregunta_procesada = self.preprocess_text(pregunta)\n",
        "                self.all_questions.append(pregunta_procesada)\n",
        "                self.question_to_answer[pregunta_procesada] = respuesta\n",
        "\n",
        "        print(f\"\\n✓ Total de preguntas en la base: {len(self.all_questions)}\")\n",
        "        print(f\"✓ Total de respuestas únicas: {len(self.qa_pairs)}\")\n",
        "\n",
        "        # Vectorizar preguntas usando TF-IDF\n",
        "        print(\"\\n Vectorizando preguntas con TF-IDF...\")\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.question_vectors = self.vectorizer.fit_transform(self.all_questions)\n",
        "\n",
        "        print(\"✓ Vectorización completada\")\n",
        "\n",
        "    def get_response(self, user_question, threshold=0.3):\n",
        "        \"\"\"\n",
        "        Obtiene la respuesta más similar a la pregunta del usuario\n",
        "\n",
        "        Args:\n",
        "            user_question: Pregunta del usuario\n",
        "            threshold: Umbral mínimo de similitud (0-1)\n",
        "\n",
        "        Returns:\n",
        "            Respuesta del chatbot y score de confianza\n",
        "        \"\"\"\n",
        "        # Preprocesar pregunta del usuario\n",
        "        processed_question = self.preprocess_text(user_question)\n",
        "\n",
        "        # Vectorizar pregunta del usuario\n",
        "        user_vector = self.vectorizer.transform([processed_question])\n",
        "\n",
        "        # Calcular similitud con todas las preguntas\n",
        "        similarities = cosine_similarity(user_vector, self.question_vectors)[0]\n",
        "\n",
        "        # Encontrar la pregunta más similar\n",
        "        best_match_idx = np.argmax(similarities)\n",
        "        best_similarity = similarities[best_match_idx]\n",
        "\n",
        "        # Si la similitud es muy baja, no sabemos la respuesta\n",
        "        if best_similarity < threshold:\n",
        "            return {\n",
        "                \"answer\": \"Lo siento, no tengo información sobre eso. Intenta preguntarme sobre conceptos de Machine Learning como overfitting, redes neuronales, gradient descent, etc.\",\n",
        "                \"confidence\": float(best_similarity),\n",
        "                \"matched_question\": None\n",
        "            }\n",
        "\n",
        "        # Obtener la respuesta\n",
        "        matched_question = self.all_questions[best_match_idx]\n",
        "        answer = self.question_to_answer[matched_question]\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"confidence\": float(best_similarity),\n",
        "            \"matched_question\": matched_question\n",
        "        }\n",
        "\n",
        "    def save(self, filename=\"chatbot_data.pkl\"):\n",
        "        \"\"\"\n",
        "        Guarda el chatbot completo\n",
        "        \"\"\"\n",
        "        print(f\"\\n💾 Guardando chatbot en {filename}...\")\n",
        "        joblib.dump({\n",
        "            'vectorizer': self.vectorizer,\n",
        "            'question_vectors': self.question_vectors,\n",
        "            'all_questions': self.all_questions,\n",
        "            'question_to_answer': self.question_to_answer,\n",
        "            'lemmatizer': self.lemmatizer,\n",
        "            'stop_words': self.stop_words\n",
        "        }, filename)\n",
        "        print(f\"✅ Chatbot guardado exitosamente\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Función principal para crear y guardar el chatbot\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"█\" * 60)\n",
        "    print(\" CREACIÓN DE CHATBOT ML - BASE DE CONOCIMIENTO\")\n",
        "    print(\"█\" * 60)\n",
        "\n",
        "    # Crear chatbot\n",
        "    chatbot = ChatbotMLFAQ()\n",
        "\n",
        "    # Construir base de conocimiento\n",
        "    chatbot.build_knowledge_base()\n",
        "\n",
        "    # Guardar chatbot\n",
        "    chatbot.save()\n",
        "\n",
        "    # Prueba rápida\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PRUEBA RÁPIDA DEL CHATBOT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    test_questions = [\n",
        "        \"¿Qué es machine learning?\",\n",
        "        \"Explícame overfitting\",\n",
        "        \"¿Cómo funciona gradient descent?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\n❓ Pregunta: {question}\")\n",
        "        response = chatbot.get_response(question)\n",
        "        print(f\"🤖 Respuesta: {response['answer'][:100]}...\")\n",
        "        print(f\"📊 Confianza: {response['confidence']:.2%}\")\n",
        "\n",
        "    print(\"\\n\" + \"█\" * 60)\n",
        "    print(\" ✅ CHATBOT CREADO Y GUARDADO EXITOSAMENTE\")\n",
        "    print(\"█\" * 60)\n",
        "    print(\"\\nAhora ejecuta: python 02_api_chatbot.py\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870367f4-8530-4761-b9a8-e3e34c25ff8f",
        "id": "54rbr9IWlyRc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CONFIGURACIÓN INICIAL\n",
            "============================================================\n",
            "\n",
            "Descargando recursos de NLTK (puede tardar unos segundos)...\n",
            "  ✓ punkt descargado\n",
            "  ✓ punkt_tab descargado\n",
            "  ✓ stopwords descargado\n",
            "  ✓ wordnet descargado\n",
            "  ✓ omw-1.4 descargado\n",
            "\n",
            "✅ Recursos de NLTK listos\n",
            "\n",
            "\n",
            "████████████████████████████████████████████████████████████\n",
            " CREACIÓN DE CHATBOT ML - BASE DE CONOCIMIENTO\n",
            "████████████████████████████████████████████████████████████\n",
            "\n",
            "============================================================\n",
            "CONSTRUYENDO BASE DE CONOCIMIENTO\n",
            "============================================================\n",
            "\n",
            "✓ Total de preguntas en la base: 56\n",
            "✓ Total de respuestas únicas: 14\n",
            "\n",
            " Vectorizando preguntas con TF-IDF...\n",
            "✓ Vectorización completada\n",
            "\n",
            "💾 Guardando chatbot en chatbot_data.pkl...\n",
            "✅ Chatbot guardado exitosamente\n",
            "\n",
            "============================================================\n",
            "PRUEBA RÁPIDA DEL CHATBOT\n",
            "============================================================\n",
            "\n",
            "❓ Pregunta: ¿Qué es machine learning?\n",
            "🤖 Respuesta: Machine Learning es una rama de la Inteligencia Artificial que permite a las máquinas aprender de lo...\n",
            "📊 Confianza: 100.00%\n",
            "\n",
            "❓ Pregunta: Explícame overfitting\n",
            "🤖 Respuesta: Overfitting ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el...\n",
            "📊 Confianza: 100.00%\n",
            "\n",
            "❓ Pregunta: ¿Cómo funciona gradient descent?\n",
            "🤖 Respuesta: Gradient Descent es un algoritmo de optimización que ajusta los parámetros de un modelo para minimiz...\n",
            "📊 Confianza: 85.50%\n",
            "\n",
            "████████████████████████████████████████████████████████████\n",
            " ✅ CHATBOT CREADO Y GUARDADO EXITOSAMENTE\n",
            "████████████████████████████████████████████████████████████\n",
            "\n",
            "Ahora ejecuta: python 02_api_chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 🌐 PASO 3: API REST del Chatbot\n",
        "\n",
        "**Salida esperada:**\n",
        "```\n",
        "INICIANDO API DEL CHATBOT ML\n",
        "📂 Cargando chatbot desde chatbot_data.pkl...\n",
        "✅ Chatbot cargado exitosamente\n",
        "   - Preguntas en base: 45\n",
        "\n",
        "Iniciando servidor en: http://127.0.0.1:8000\n",
        "Documentación: http://127.0.0.1:8000/docs"
      ],
      "metadata": {
        "id": "snkxoUCErsJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 🌐 PASO 3: API REST del Chatbot\n",
        "\n",
        "### Archivo: `02_api_chatbot.py`\n",
        "\n",
        "\"\"\"\n",
        "PASO 2: API REST PARA EL CHATBOT\n",
        "Expone el chatbot a través de una API REST con FastAPI\n",
        "\"\"\"\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "import joblib\n",
        "from typing import Optional\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURACIÓN DE LA API\n",
        "# ============================================================================\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"ML FAQ Chatbot API\",\n",
        "    description=\"API de chatbot para preguntas sobre Machine Learning\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Configurar CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Variable global para el chatbot\n",
        "chatbot_data = None\n",
        "\n",
        "# ============================================================================\n",
        "# MODELOS DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    \"\"\"\n",
        "    Modelo de entrada: pregunta del usuario\n",
        "    \"\"\"\n",
        "    question: str = Field(\n",
        "        ...,\n",
        "        description=\"Pregunta del usuario sobre Machine Learning\",\n",
        "        min_length=1,\n",
        "        max_length=500\n",
        "    )\n",
        "    threshold: Optional[float] = Field(\n",
        "        0.3,\n",
        "        description=\"Umbral mínimo de confianza (0-1)\",\n",
        "        ge=0.0,\n",
        "        le=1.0\n",
        "    )\n",
        "\n",
        "    class Config:\n",
        "        json_schema_extra = {\n",
        "            \"example\": {\n",
        "                \"question\": \"¿Qué es overfitting?\",\n",
        "                \"threshold\": 0.3\n",
        "            }\n",
        "        }\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    \"\"\"\n",
        "    Modelo de respuesta del chatbot\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    confidence: float\n",
        "    matched_question: Optional[str]\n",
        "    status: str\n",
        "\n",
        "# ============================================================================\n",
        "# CLASE CHATBOT (versión para API)\n",
        "# ============================================================================\n",
        "\n",
        "class ChatbotAPI:\n",
        "    \"\"\"\n",
        "    Versión del chatbot para usar en la API\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_file=\"chatbot_data.pkl\"):\n",
        "        \"\"\"\n",
        "        Carga el chatbot desde el archivo guardado\n",
        "        \"\"\"\n",
        "        if not os.path.exists(data_file):\n",
        "            raise FileNotFoundError(\n",
        "                f\"No se encontró {data_file}. \"\n",
        "                f\"Ejecuta primero: python 01_crear_base_conocimiento.py\"\n",
        "            )\n",
        "\n",
        "        print(f\"📂 Cargando chatbot desde {data_file}...\")\n",
        "        data = joblib.load(data_file)\n",
        "\n",
        "        self.vectorizer = data['vectorizer']\n",
        "        self.question_vectors = data['question_vectors']\n",
        "        self.all_questions = data['all_questions']\n",
        "        self.question_to_answer = data['question_to_answer']\n",
        "        self.lemmatizer = data['lemmatizer']\n",
        "        self.stop_words = data['stop_words']\n",
        "\n",
        "        print(\"✅ Chatbot cargado exitosamente\")\n",
        "        print(f\"   - Preguntas en base: {len(self.all_questions)}\")\n",
        "        print(f\"   - Respuestas únicas: {len(set(self.question_to_answer.values()))}\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesa el texto\n",
        "        \"\"\"\n",
        "        from nltk.tokenize import word_tokenize\n",
        "\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text, language='spanish')\n",
        "        processed_tokens = [\n",
        "            self.lemmatizer.lemmatize(token)\n",
        "            for token in tokens\n",
        "            if token.isalnum() and token not in self.stop_words\n",
        "        ]\n",
        "        return ' '.join(processed_tokens)\n",
        "\n",
        "    def get_response(self, user_question, threshold=0.3):\n",
        "        \"\"\"\n",
        "        Obtiene respuesta para la pregunta del usuario\n",
        "        \"\"\"\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        import numpy as np\n",
        "\n",
        "        # Preprocesar pregunta\n",
        "        processed_question = self.preprocess_text(user_question)\n",
        "\n",
        "        # Vectorizar\n",
        "        user_vector = self.vectorizer.transform([processed_question])\n",
        "\n",
        "        # Calcular similitud\n",
        "        similarities = cosine_similarity(user_vector, self.question_vectors)[0]\n",
        "\n",
        "        # Mejor coincidencia\n",
        "        best_match_idx = np.argmax(similarities)\n",
        "        best_similarity = similarities[best_match_idx]\n",
        "\n",
        "        # Verificar umbral\n",
        "        if best_similarity < threshold:\n",
        "            return {\n",
        "                \"answer\": \"Lo siento, no tengo información sobre eso. Intenta preguntarme sobre conceptos de Machine Learning como overfitting, redes neuronales, gradient descent, Random Forest, etc.\",\n",
        "                \"confidence\": float(best_similarity),\n",
        "                \"matched_question\": None,\n",
        "                \"status\": \"low_confidence\"\n",
        "            }\n",
        "\n",
        "        # Obtener respuesta\n",
        "        matched_question = self.all_questions[best_match_idx]\n",
        "        answer = self.question_to_answer[matched_question]\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"confidence\": float(best_similarity),\n",
        "            \"matched_question\": matched_question,\n",
        "            \"status\": \"success\"\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# EVENTOS DE LA API\n",
        "# ============================================================================\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    \"\"\"\n",
        "    Carga el chatbot al iniciar la API\n",
        "    \"\"\"\n",
        "    global chatbot_data\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INICIANDO API DEL CHATBOT ML\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        chatbot_data = ChatbotAPI()\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\n❌ ERROR: {e}\\n\")\n",
        "        raise\n",
        "\n",
        "# ============================================================================\n",
        "# ENDPOINTS\n",
        "# ============================================================================\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"\n",
        "    Endpoint raíz - información de la API\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"message\": \"ML FAQ Chatbot API\",\n",
        "        \"version\": \"1.0.0\",\n",
        "        \"description\": \"Chatbot especializado en preguntas sobre Machine Learning\",\n",
        "        \"endpoints\": {\n",
        "            \"chat\": \"/chat (POST)\",\n",
        "            \"health\": \"/health (GET)\",\n",
        "            \"stats\": \"/stats (GET)\",\n",
        "            \"examples\": \"/examples (GET)\",\n",
        "            \"docs\": \"/docs (GET)\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    \"\"\"\n",
        "    Verificación de salud de la API\n",
        "    \"\"\"\n",
        "    if chatbot_data is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Chatbot no cargado\")\n",
        "\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"chatbot_loaded\": True,\n",
        "        \"questions_in_base\": len(chatbot_data.all_questions)\n",
        "    }\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "async def stats():\n",
        "    \"\"\"\n",
        "    Estadísticas del chatbot\n",
        "    \"\"\"\n",
        "    if chatbot_data is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Chatbot no cargado\")\n",
        "\n",
        "    return {\n",
        "        \"total_questions\": len(chatbot_data.all_questions),\n",
        "        \"unique_answers\": len(set(chatbot_data.question_to_answer.values())),\n",
        "        \"topics\": [\n",
        "            \"Machine Learning básico\",\n",
        "            \"Overfitting y validación\",\n",
        "            \"Algoritmos (Random Forest, Gradient Descent)\",\n",
        "            \"Métricas (Accuracy)\",\n",
        "            \"Conceptos (Feature Engineering, Hiperparámetros)\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "@app.get(\"/examples\")\n",
        "async def examples():\n",
        "    \"\"\"\n",
        "    Ejemplos de preguntas que el chatbot puede responder\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"examples\": [\n",
        "            \"¿Qué es machine learning?\",\n",
        "            \"Explícame qué es overfitting\",\n",
        "            \"¿Qué diferencia hay entre clasificación y regresión?\",\n",
        "            \"¿Qué es gradient descent?\",\n",
        "            \"¿Para qué sirve cross-validation?\",\n",
        "            \"¿Qué es Random Forest?\",\n",
        "            \"Explica feature engineering\",\n",
        "            \"¿Qué es un hiperparámetro?\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat(request: ChatRequest):\n",
        "    \"\"\"\n",
        "    Endpoint principal del chatbot\n",
        "\n",
        "    Recibe una pregunta y devuelve la respuesta del chatbot\n",
        "    \"\"\"\n",
        "    if chatbot_data is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Chatbot no cargado\")\n",
        "\n",
        "    try:\n",
        "        # Obtener respuesta del chatbot\n",
        "        response = chatbot_data.get_response(\n",
        "            request.question,\n",
        "            threshold=request.threshold\n",
        "        )\n",
        "\n",
        "        return ChatResponse(\n",
        "            question=request.question,\n",
        "            answer=response[\"answer\"],\n",
        "            confidence=response[\"confidence\"],\n",
        "            matched_question=response[\"matched_question\"],\n",
        "            status=response[\"status\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=f\"Error al procesar la pregunta: {str(e)}\"\n",
        "        )\n",
        "\n",
        "# ============================================================================\n",
        "# EJECUTAR SERVIDOR\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SERVIDOR API - ML FAQ Chatbot\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nIniciando servidor en: http://127.0.0.1:8000\")\n",
        "    print(\"Documentación: http://127.0.0.1:8000/docs\")\n",
        "    print(\"\\nPresiona CTRL+C para detener\\n\")\n",
        "\n",
        "    # Configuración para evitar conflictos con event loops existentes\n",
        "    import sys\n",
        "\n",
        "    # Detectar si estamos en Jupyter/Colab\n",
        "    try:\n",
        "        get_ipython()\n",
        "        IN_NOTEBOOK = True\n",
        "    except NameError:\n",
        "        IN_NOTEBOOK = False\n",
        "\n",
        "    if IN_NOTEBOOK:\n",
        "        # En Jupyter/Colab, usar nest_asyncio\n",
        "        print(\"⚠️  Detectado entorno Jupyter/Colab\")\n",
        "        print(\"   Ejecutando servidor en modo compatible...\\n\")\n",
        "\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "        except ImportError:\n",
        "            print(\"❌ Instalando nest_asyncio...\")\n",
        "            import subprocess\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nest_asyncio\"])\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "\n",
        "        # Iniciar servidor en segundo plano\n",
        "        import threading\n",
        "\n",
        "        def run_server():\n",
        "            uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "        server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "        server_thread.start()\n",
        "\n",
        "        print(\"✅ Servidor iniciado en segundo plano\")\n",
        "        print(\"   Accede a: http://127.0.0.1:8000/docs\")\n",
        "        print(\"   El servidor se detendrá cuando cierres el notebook\\n\")\n",
        "    else:\n",
        "        # En terminal normal\n",
        "        uvicorn.run(\n",
        "            app,\n",
        "            host=\"0.0.0.0\",\n",
        "            port=8000,\n",
        "            log_level=\"info\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e229a32-dbd8-47e4-f83d-4f579a55f26c",
        "id": "d_Fzo9JkmEnn"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4206241009.py:171: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SERVIDOR API - ML FAQ Chatbot\n",
            "============================================================\n",
            "\n",
            "Iniciando servidor en: http://127.0.0.1:8000\n",
            "Documentación: http://127.0.0.1:8000/docs\n",
            "\n",
            "Presiona CTRL+C para detener\n",
            "\n",
            "⚠️  Detectado entorno Jupyter/Colab\n",
            "   Ejecutando servidor en modo compatible...\n",
            "\n",
            "✅ Servidor iniciado en segundo plano\n",
            "   Accede a: http://127.0.0.1:8000/docs\n",
            "   El servidor se detendrá cuando cierres el notebook\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 🧪 PASO 4: Cliente de Prueba"
      ],
      "metadata": {
        "id": "xmGYeJN7r5O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 🧪 PASO 4: Cliente de Prueba\n",
        "\n",
        "### Archivo: `03_test_chatbot.py`\n",
        "\"\"\"\n",
        "PASO 3: CLIENTE PARA PROBAR EL CHATBOT\n",
        "Script para probar la API del chatbot\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "BASE_URL = \"http://127.0.0.1:8000\"\n",
        "\n",
        "def print_response(title, response):\n",
        "    \"\"\"\n",
        "    Imprime la respuesta de manera bonita\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"  {title}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Status: {response.status_code}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(json.dumps(data, indent=2, ensure_ascii=False))\n",
        "    else:\n",
        "        print(f\"Error: {response.text}\")\n",
        "\n",
        "def test_health():\n",
        "    \"\"\"\n",
        "    Test del health check\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"{BASE_URL}/health\")\n",
        "    print_response(\"TEST 1: Health Check\", response)\n",
        "\n",
        "def test_stats():\n",
        "    \"\"\"\n",
        "    Test de estadísticas\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"{BASE_URL}/stats\")\n",
        "    print_response(\"TEST 2: Estadísticas del Chatbot\", response)\n",
        "\n",
        "def test_examples():\n",
        "    \"\"\"\n",
        "    Test de ejemplos\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"{BASE_URL}/examples\")\n",
        "    print_response(\"TEST 3: Preguntas de Ejemplo\", response)\n",
        "\n",
        "def test_chat_questions():\n",
        "    \"\"\"\n",
        "    Test de preguntas al chatbot\n",
        "    \"\"\"\n",
        "    questions = [\n",
        "        \"¿Qué es machine learning?\",\n",
        "        \"Explícame el overfitting por favor\",\n",
        "        \"¿Cuál es la diferencia entre clasificación y regresión?\",\n",
        "        \"¿Qué es el gradient descent?\",\n",
        "        \"Hola chatbot\",\n",
        "        \"Gracias por la ayuda\",\n",
        "        \"¿Qué es la física cuántica?\"  # Esta NO debería saber\n",
        "    ]\n",
        "\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"  TEST 4.{i}: Pregunta al Chatbot\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"❓ Pregunta: {question}\")\n",
        "\n",
        "        response = requests.post(\n",
        "            f\"{BASE_URL}/chat\",\n",
        "            json={\"question\": question}\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"\\n🤖 Respuesta:\")\n",
        "            print(f\"   {data['answer']}\")\n",
        "            print(f\"\\n📊 Confianza: {data['confidence']:.2%}\")\n",
        "            print(f\"📌 Estado: {data['status']}\")\n",
        "            if data['matched_question']:\n",
        "                print(f\"🔍 Pregunta similar: {data['matched_question']}\")\n",
        "        else:\n",
        "            print(f\"❌ Error: {response.text}\")\n",
        "\n",
        "def run_all_tests():\n",
        "    \"\"\"\n",
        "    Ejecuta todos los tests\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"█\" * 70)\n",
        "    print(\"  SUITE DE TESTS - ML FAQ CHATBOT API\")\n",
        "    print(\"█\" * 70)\n",
        "\n",
        "    try:\n",
        "        test_health()\n",
        "        test_stats()\n",
        "        test_examples()\n",
        "        test_chat_questions()\n",
        "\n",
        "        print(\"\\n\" + \"█\" * 70)\n",
        "        print(\"  ✅ TODOS LOS TESTS COMPLETADOS\")\n",
        "        print(\"█\" * 70 + \"\\n\")\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"\\n❌ ERROR: No se puede conectar a la API\")\n",
        "        print(\"Asegúrate de que el servidor esté corriendo:\")\n",
        "        print(\"   python 02_api_chatbot.py\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERROR: {e}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8d47cb-ccd6-49d5-aad3-62d6c7cd4e74",
        "id": "iLwGXTrYmRei"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [3110]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INICIANDO API DEL CHATBOT ML\n",
            "============================================================\n",
            "\n",
            "📂 Cargando chatbot desde chatbot_data.pkl...\n",
            "✅ Chatbot cargado exitosamente\n",
            "   - Preguntas en base: 56\n",
            "   - Respuestas únicas: 14\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "  SUITE DE TESTS - ML FAQ CHATBOT API\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "INFO:     127.0.0.1:35750 - \"GET /health HTTP/1.1\" 200 OK\n",
            "\n",
            "======================================================================\n",
            "  TEST 1: Health Check\n",
            "======================================================================\n",
            "Status: 200\n",
            "{\n",
            "  \"status\": \"healthy\",\n",
            "  \"chatbot_loaded\": true,\n",
            "  \"questions_in_base\": 56\n",
            "}\n",
            "INFO:     127.0.0.1:35762 - \"GET /stats HTTP/1.1\" 200 OK\n",
            "\n",
            "======================================================================\n",
            "  TEST 2: Estadísticas del Chatbot\n",
            "======================================================================\n",
            "Status: 200\n",
            "{\n",
            "  \"total_questions\": 56,\n",
            "  \"unique_answers\": 14,\n",
            "  \"topics\": [\n",
            "    \"Machine Learning básico\",\n",
            "    \"Overfitting y validación\",\n",
            "    \"Algoritmos (Random Forest, Gradient Descent)\",\n",
            "    \"Métricas (Accuracy)\",\n",
            "    \"Conceptos (Feature Engineering, Hiperparámetros)\"\n",
            "  ]\n",
            "}\n",
            "INFO:     127.0.0.1:35768 - \"GET /examples HTTP/1.1\" 200 OK\n",
            "\n",
            "======================================================================\n",
            "  TEST 3: Preguntas de Ejemplo\n",
            "======================================================================\n",
            "Status: 200\n",
            "{\n",
            "  \"examples\": [\n",
            "    \"¿Qué es machine learning?\",\n",
            "    \"Explícame qué es overfitting\",\n",
            "    \"¿Qué diferencia hay entre clasificación y regresión?\",\n",
            "    \"¿Qué es gradient descent?\",\n",
            "    \"¿Para qué sirve cross-validation?\",\n",
            "    \"¿Qué es Random Forest?\",\n",
            "    \"Explica feature engineering\",\n",
            "    \"¿Qué es un hiperparámetro?\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.1: Pregunta al Chatbot\n",
            "======================================================================\n",
            "❓ Pregunta: ¿Qué es machine learning?\n",
            "INFO:     127.0.0.1:35774 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "🤖 Respuesta:\n",
            "   Machine Learning es una rama de la Inteligencia Artificial que permite a las máquinas aprender de los datos sin ser programadas explícitamente. Los algoritmos identifican patrones y toman decisiones basándose en ejemplos anteriores.\n",
            "\n",
            "📊 Confianza: 100.00%\n",
            "📌 Estado: success\n",
            "🔍 Pregunta similar: machine learning\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.2: Pregunta al Chatbot\n",
            "======================================================================\n",
            "❓ Pregunta: Explícame el overfitting por favor\n",
            "INFO:     127.0.0.1:35784 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "🤖 Respuesta:\n",
            "   Overfitting ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido. Esto hace que funcione muy bien con los datos de entrenamiento pero mal con datos nuevos. Se soluciona con validación cruzada, regularización o más datos.\n",
            "\n",
            "📊 Confianza: 100.00%\n",
            "📌 Estado: success\n",
            "🔍 Pregunta similar: overfitting\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.3: Pregunta al Chatbot\n",
            "======================================================================\n",
            "❓ Pregunta: ¿Cuál es la diferencia entre clasificación y regresión?\n",
            "INFO:     127.0.0.1:35796 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "🤖 Respuesta:\n",
            "   La clasificación predice categorías o clases (ej: spam/no spam, gato/perro), mientras que la regresión predice valores numéricos continuos (ej: precio de una casa, temperatura). Clasificación da etiquetas, regresión da números.\n",
            "\n",
            "📊 Confianza: 100.00%\n",
            "📌 Estado: success\n",
            "🔍 Pregunta similar: diferencia clasificación regresión\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.4: Pregunta al Chatbot\n",
            "======================================================================\n",
            "❓ Pregunta: ¿Qué es el gradient descent?\n",
            "INFO:     127.0.0.1:35802 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "🤖 Respuesta:\n",
            "   Gradient Descent es un algoritmo de optimización que ajusta los parámetros de un modelo para minimizar el error. Funciona calculando el gradiente (derivada) de la función de pérdida y moviéndose en la dirección opuesta para encontrar el mínimo.\n",
            "\n",
            "📊 Confianza: 100.00%\n",
            "📌 Estado: success\n",
            "🔍 Pregunta similar: gradient descent\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.5: Pregunta al Chatbot\n",
            "======================================================================\n",
            "❓ Pregunta: Hola chatbot\n",
            "INFO:     127.0.0.1:35804 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "🤖 Respuesta:\n",
            "   ¡Hola! Soy un chatbot especializado en Machine Learning. Puedo responder preguntas sobre conceptos de ML, algoritmos, métricas y más. ¿En qué puedo ayudarte?\n",
            "\n",
            "📊 Confianza: 100.00%\n",
            "📌 Estado: success\n",
            "🔍 Pregunta similar: hola\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.6: Pregunta al Chatbot\n",
            "======================================================================\n",
            "❓ Pregunta: Gracias por la ayuda\n",
            "INFO:     127.0.0.1:35814 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "🤖 Respuesta:\n",
            "   ¡De nada! Si tienes más preguntas sobre Machine Learning, estaré encantado de ayudarte. 😊\n",
            "\n",
            "📊 Confianza: 100.00%\n",
            "📌 Estado: success\n",
            "🔍 Pregunta similar: gracias\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.7: Pregunta al Chatbot\n",
            "======================================================================\n",
            "❓ Pregunta: ¿Qué es la física cuántica?\n",
            "INFO:     127.0.0.1:35822 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "🤖 Respuesta:\n",
            "   Lo siento, no tengo información sobre eso. Intenta preguntarme sobre conceptos de Machine Learning como overfitting, redes neuronales, gradient descent, Random Forest, etc.\n",
            "\n",
            "📊 Confianza: 0.00%\n",
            "📌 Estado: low_confidence\n",
            "\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "  ✅ TODOS LOS TESTS COMPLETADOS\n",
            "██████████████████████████████████████████████████████████████████████\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 🧪 PASO 5: Lanzar el servidor en local"
      ],
      "metadata": {
        "id": "B1VSOAVqr_IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EXPONER SERVIDOR EN GOOGLE COLAB\n",
        "# ========================================\n",
        "\n",
        "# Usar el proxy de Colab\n",
        "from google.colab.output import eval_js\n",
        "print(\"🔄 Obteniendo URL del servidor...\")\n",
        "\n",
        "# La URL base de Colab\n",
        "colab_url = eval_js(\"google.colab.kernel.proxyPort(8000)\")\n",
        "\n",
        "print(\"\\n\" + \"✅\" * 35)\n",
        "print(\"\\n   🎉 SERVIDOR ACTIVO EN COLAB 🎉\")\n",
        "print(\"\\n\" + \"✅\" * 35)\n",
        "print(f\"\\n🌐 URL del servidor:\")\n",
        "print(f\"   {colab_url}\")\n",
        "print(f\"\\n📖 ABRE ESTA URL EN TU NAVEGADOR:\")\n",
        "print(f\"   👉 {colab_url}/docs\")\n",
        "print(f\"\\n📍 Otros endpoints:\")\n",
        "print(f\"   • Health:   {colab_url}/health\")\n",
        "print(f\"   • Chat:     {colab_url}/chat\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\\n💡 Click en la URL para abrir en nueva pestaña\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "E6na00WNmDo8",
        "outputId": "32350407-64c9-482d-fa77-5050cf188624"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Obteniendo URL del servidor...\n",
            "\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "   🎉 SERVIDOR ACTIVO EN COLAB 🎉\n",
            "\n",
            "✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅\n",
            "\n",
            "🌐 URL del servidor:\n",
            "   https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev\n",
            "\n",
            "📖 ABRE ESTA URL EN TU NAVEGADOR:\n",
            "   👉 https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev/docs\n",
            "\n",
            "📍 Otros endpoints:\n",
            "   • Health:   https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev/health\n",
            "   • Chat:     https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev/chat\n",
            "\n",
            "======================================================================\n",
            "\n",
            "💡 Click en la URL para abrir en nueva pestaña\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 💡 CÓMO FUNCIONA EL CHATBOT\n",
        "\n",
        "### 1. Procesamiento de Texto (NLP)\n",
        "\n",
        "```python\n",
        "Pregunta original: \"¿Qué es el overfitting?\"\n",
        "                    ↓\n",
        "Preprocesamiento:  - Minúsculas\n",
        "                   - Tokenización\n",
        "                   - Eliminar stopwords (\"qué\", \"es\", \"el\")\n",
        "                   - Lematización\n",
        "                    ↓\n",
        "Texto procesado:   \"overfitting\"\n",
        "```\n",
        "\n",
        "### 2. Vectorización TF-IDF\n",
        "\n",
        "```\n",
        "TF-IDF convierte texto en números:\n",
        "\n",
        "\"overfitting\" → [0.0, 0.0, 0.87, 0.0, 0.0, ...]\n",
        "                 (vector de 100+ dimensiones)\n",
        "```\n",
        "\n",
        "### 3. Similitud Coseno\n",
        "\n",
        "```python\n",
        "Pregunta usuario:    [0.0, 0.0, 0.87, ...]\n",
        "                            ↓\n",
        "         Calcular similitud con TODAS las preguntas\n",
        "                            ↓\n",
        "Pregunta 1: 0.23 ←\n",
        "Pregunta 2: 0.95 ← ¡MEJOR MATCH!\n",
        "Pregunta 3: 0.15 ←\n",
        "                            ↓\n",
        "            Devolver respuesta de Pregunta 2\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_Y2LKYissh4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🎓 EJERCICIOS INTERESANTES\n",
        "\n",
        "### Ejercicio 1: Añadir más preguntas (Fácil)\n",
        "Modifica `01_crear_base_conocimiento.py` para añadir 3 nuevas preguntas sobre Deep Learning.\n",
        "\n",
        "### Ejercicio 2: Ajustar el threshold (Medio)\n",
        "Experimenta cambiando el threshold en `/chat`. ¿Qué pasa si lo subes a 0.7? ¿Y si lo bajas a 0.1?\n",
        "\n",
        "### Ejercicio 3: Endpoint de feedback (Medio)\n",
        "Crea un nuevo endpoint `/feedback` que permita al usuario indicar si la respuesta fue útil.\n",
        "\n",
        "### Ejercicio 4: Historial de conversación (Avanzado)\n",
        "Implementa un sistema que guarde el historial de preguntas en memoria para mantener contexto.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5gd74GLUssXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ CHECKLIST DE LA SESIÓN\n",
        "\n",
        "- [ ] Entender qué es industrialización de modelos\n",
        "- [ ] Conocer conceptos básicos de NLP (tokenización, TF-IDF)\n",
        "- [ ] Comprender cómo funciona similitud coseno\n",
        "- [ ] Crear una base de conocimiento\n",
        "- [ ] Serializar datos con pickle\n",
        "- [ ] Crear una API REST con FastAPI\n",
        "- [ ] Probar la API con diferentes métodos\n",
        "- [ ] Explorar documentación automática (Swagger)\n",
        "- [ ] Modificar y extender el chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 RECURSOS ADICIONALES\n",
        "\n",
        "### Documentación:\n",
        "- **NLTK**: https://www.nltk.org/\n",
        "- **Scikit-learn TfidfVectorizer**: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "- **FastAPI**: https://fastapi.tiangolo.com/\n",
        "- **Cosine Similarity**: https://en.wikipedia.org/wiki/Cosine_similarity\n",
        "\n",
        "### Tutoriales:\n",
        "- Introduction to NLP with NLTK\n",
        "- Building REST APIs with FastAPI\n",
        "- Text Similarity with TF-IDF"
      ],
      "metadata": {
        "id": "BczROsC6tMVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎓 PREGUNTAS FRECUENTES\n",
        "\n",
        "**P: ¿Por qué usar TF-IDF y no simplemente contar palabras?**\n",
        "R: TF-IDF da más peso a palabras importantes y menos a palabras comunes. \"Overfitting\" es más importante que \"qué\" o \"es\".\n",
        "\n",
        "**P: ¿Esto es un modelo de Machine Learning real?**\n",
        "R: No es un modelo que \"aprende\", pero usa técnicas de ML (vectorización, similitud). Es perfecto para entender industrialización.\n",
        "\n",
        "**P: ¿Puedo usarlo para otros idiomas?**\n",
        "R: Sí, solo cambia `language='spanish'` por `language='english'` en el código y ajusta los stopwords.\n",
        "\n",
        "**P: ¿Cómo lo escalo para miles de preguntas?**\n",
        "R: Considera usar Elasticsearch o FAISS para búsquedas vectoriales más eficientes.\n",
        "\n",
        "**P: ¿Funciona con WhatsApp/Telegram?**\n",
        "R: Sí, puedes integrar la API con cualquier plataforma usando sus APIs (ej: Twilio para WhatsApp).\n"
      ],
      "metadata": {
        "id": "-aNoNxGTtWCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎉 MENSAJE FINAL\n",
        "\n",
        "Este proyecto demuestra que industrializar un modelo no siempre requiere infraestructura compleja. Con FastAPI + pickle + un poco de NLP, puedes crear un servicio útil en menos de 200 líneas de código.\n",
        "\n",
        "**Lo más importante:** Los estudiantes ven el ciclo completo:\n",
        "1. Crear conocimiento → 2. Guardarlo → 3. Exponerlo como API → 4. Consumirlo\n",
        "\n",
        "¡Esto es industrialización en su forma más práctica!"
      ],
      "metadata": {
        "id": "rXYA0DXEtLi4"
      }
    }
  ]
}